{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<div style=\"border: 6px solid #18453B; padding: 10px; border-radius: 5px; text-align: center;\">\n",
    "   <img src=\"https://miro.medium.com/v2/resize:fit:2000/1*N1-K-A43_98pYZ27fnupDA.jpeg\" alt=\"Placeholder image\" width=900>\n",
    "    <h1 style=\"color: #4CAF50;\">Homework 06:<br>Linear Regression & Projects</h1>\n",
    "    <p>Let's learn a bit more about linear regression!</p>\n",
    " \n",
    "</div>\n",
    "\n",
    "\n",
    "### Instructions for Submitting Names in Notebooks\n",
    "\n",
    "Please follow the format below when typing your names in the notebook. \n",
    "\n",
    "- Each member's name must be written in the format:  \n",
    "  **Last Name, First Name, Second Name**\n",
    "\n",
    "#### Example:\n",
    "    Poe, Edgar Allan\n",
    "\n",
    "> ⚠️ **Failure to follow this format will result in a reduction of your grade.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "names"
    ]
   },
   "source": [
    "_Name_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#33AA44>Problem #1 (10pts)</font>\n",
    "\n",
    "![loss](https://miro.medium.com/max/574/1*mOjTP97xJacJLEAQx_L9eA.png)\n",
    "\n",
    "\n",
    "Derive the basic equations given in class for linear regression (LR) in 1D. Show all of your work. In the next markdown cell and use [$\\LaTeX$](https://ashki23.github.io/markdown-latex.html) for the equations.\n",
    "\n",
    "That is, use the MSE loss function:\n",
    "$${\\cal L}(m,b) = \\frac{1}{2}\\sum_d \\left(y_d - mx_d - b \\right)^2,$$\n",
    "and the minimization condition\n",
    "$$\\frac{\\partial {\\cal L}}{\\partial m} = 0,$$\n",
    "$$\\frac{\\partial {\\cal L}}{\\partial b} = 0.$$\n",
    "Hopefully you remember how to do partial derivatives! If not, contact us right away! These mathematical steps minimize the loss function and are the statement of _optimization_: see the picture above.\n",
    "\n",
    "When you do this you should end up with two algebraic equations that involve sums over the data. It might look \"messy\" - no problem. Show all of your work. What you want to do is solve for $m$ and $b$ (I am using the slope and intercept notation in this problem): if we can write all of the sums in terms of $m$ and $b$, then we have solved the linear regression problem once and for all! We simply plug the data into the sums and out pops the \"slope\" and \"intercept\"! \n",
    "\n",
    "[Ok, what you are doing here is impressive, but there are some caveats to the \"once and for all\" statement. We can modify the loss function in many ways and we would need to redo this optimization problem again for each choice.]\n",
    "\n",
    "Next, once you have solved for $m$ and $b$, write your answer in terms of **statistical quantities** such as the variances, covariances and/or means. Note that you have two equations in two unknowns; you will have to solve those simultaneous equations to get the expressions for $m$ and $b$. \n",
    "\n",
    "There are many different notations used for these types of problems. I suggest you establish a convention you like to use and stick with it. Here is what I propose we use for now:\n",
    "\n",
    "* expectation value (mean)\n",
    "$$\\mathrm{E}[X] \\equiv \\langle X\\rangle \\equiv \\mu,$$\n",
    "$$    = \\frac{1}{N}\\sum_{d=1}^N x_d,$$\n",
    "where the $d$ sum is over the $N$ data points.\n",
    "\n",
    "* variance\n",
    "$$\\mathrm{Var}[X] = \\mathrm{E}[(X - \\mathrm{E}[X])^2],$$\n",
    "$$    = \\mathrm{E}[X^2] - \\mathrm{E}[X]^2,$$\n",
    "$$    = \\langle X^2\\rangle - \\mu^2,$$\n",
    "$$    = \\mathrm{Cov}[X,X].$$\n",
    "\n",
    "* covariance\n",
    "$$\\mathrm{Cov}[X,Y] = \\mathrm{E}[(X - \\mathrm{E}[X])(Y - \\mathrm{E}[Y])],$$\n",
    "$$    = \\mathrm{E}[XY] - \\mathrm{E}[X]\\mathrm{E}[Y].$$\n",
    "\n",
    "I tried to follow definitions used in Wikipedia so that we can all use the same conventions. Note that I have indicated some options for the mean so that you get used to seeing different forms.\n",
    "\n",
    "I wanted to comment on these conventions. There are two choices here that are not universal: the factor of $\\frac{1}{2}$ in ${\\cal L}$ and the factor of $\\frac{1}{N}$ in the statistical quantities. Neither of these choices really matters as long as you:\n",
    "* are consistent everywhere in your math,\n",
    "* use other people's libraries correctly (e.g., `statsmodels`),\n",
    "* communicate to others what you did.\n",
    "\n",
    "These choices are commonly made because they make the math cleaner, as you might have noticed already.\n",
    "\n",
    "Organize your equations so that you completely understand them. You will code them in the next problem. Now that you have these equations you can do linear regression trivially on any dataset - congrats!\n",
    "\n",
    "Here is what you should get from this problem:\n",
    "* we start with ${\\cal L}$, which in machine learning is referred to as a loss function; there are many, many types of loss functions,\n",
    "* in each case, by writing the problem in terms of a loss function, we have converted our goal directly into the mathematical problem of optimization,\n",
    "* in this problem, you go the next step to rewrite the equations you derive in terms of statistical quantities: this shows how linear regression is connected to statistics,\n",
    "* you should clearly see that we can find patterns in data using the fields of optimization and statistics; next week we will super-power this with linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Answer** Put your answer in this cell. $\\LaTeX$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#33AA44>Problem #2 (5pts)</font>\n",
    "\n",
    "![reg](https://miro.medium.com/v2/resize:fit:1400/1*Ug7uflGlIAjFe6lFMACnfA.png)\n",
    "\n",
    "Create a dataset that is a noisy line; that is, you have $y= mx+b$ and you want to add a controllable amount of noise to that using `randn` in Numpy. From the equations you derived in the first problem, use that data to compute the slope $m'$ and intercept $b'$, where the prime indicates a value inferred from the data. Also, code the equation for $R^2$. \n",
    "\n",
    "Confirm with a plot that you are getting a good fit to your data. Vary the noise, make more plots and comment on how well you are able to predict the actual values of $m$ and $b$. I recommend putting the $m$, estimated $m'$, $b$, estimated $b'$ and $R^2$ in the title of the plot with a scatterplot of the data and a solid line with your linear regression prediction. \n",
    "\n",
    "Finally, in a markdown cell, discuss how you could use the code you just wrote to handle missing values. Are there other steps you would want to add? For example, if you need to do stochastic regression, could you use the results of this homework? \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "code answer"
    ]
   },
   "source": [
    "### ANSWER"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Answer** Put your reflections and questions in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=#33AA44>Problem #3 (5pts)</font>\n",
    "![smooth](https://miro.medium.com/v2/resize:fit:1400/1*MU-few2K_ZY2gXQdSsFxaw.png)\n",
    "\n",
    "Explain the differences among regression, smoothing and finding trends, using a markdown cell. \n",
    "\n",
    "Connect these three ideas to your project. How will you use, or not use, these three ideas for your project? In general, how do these three approaches fit with our general goal of good storytelling? Can you think of a way to use all three in your project to make the message clearer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Answer** Put your reflections and questions in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color=#33AA44>Problem #4: Project (20pts)</font>\n",
    "\n",
    "![project](https://i.ytimg.com/vi/pWxDxhWXJos/maxresdefault.jpg)\n",
    "\n",
    "In this problem you will work on your project.\n",
    "\n",
    "Keep in mind that for the midterm, you do not need a complete project. You will need that by the end of the semester. The goals for the midterm project are:\n",
    "\n",
    "* you have a dataset chosen\n",
    "* you have done IDA and EDA\n",
    "* you have a good sense that this dataset will do what you want\n",
    "* you have a Github repo set up where all of your project work lives\n",
    "* you are becoming an expert on Streamlist and building apps\n",
    "* you have completed all of the data science tasks we have covered to date on this dataset (if they apply)\n",
    "    - this could include:\n",
    "        - encoding\n",
    "        - scaling\n",
    "        - imputation\n",
    "        - transforming\n",
    "        - and so on.....review the topics from each week of the course to get a list of everything we have covered so far\n",
    "* importantly, you can communicate:\n",
    "    - effective visualization, including interactive visualizations\n",
    "    - storytelling: you know your audience and you know your user (recall the ICA on knowing who you are presenting your data to)\n",
    "\n",
    "Also, recall that you can change your project after the midterm. I don't recommend this! It will be much more rewarding if you take your current work to a very complete deliverable by working on it for the entire semester. But, there are cases where this just doesn't work out (and it is totally ok!):\n",
    "* the data didn't do what you hoped it would: it is very common for us to be overly ambitious with our goals relative to what is in datasets we didn't make ourselves \n",
    "* you realize that your original idea is not that interesting after all\n",
    "* you want to stretch yourself and do a \"second\" project on something completely different: you might fall in love with _spatiotemporal_ data or _textual_ data in the next few weeks!\n",
    "\n",
    "So, what should you do for this HW problem? Here is what we will do the week before Fall Break (next week!). We will have two ICAs: no lecture. On Tuesday, you will present your project informally to your group. You will have a set of questions to ask each other and you will turn that in as your ICA. You will also teach each other \"tricks\": \n",
    "* how does each person plan to tell their story (this might not be complete until the end of the semester)\n",
    "* what visualization ideas did you learn from your group that you want to incorporate into your project?\n",
    "* what Streamlit tricks did your group share? \n",
    "\n",
    "You get the idea: that's Tuesday's ICA. Thursday, you will formally present where you are with your project to a different, random group. More instructions will be given next week, but the basic idea is that you use the wall monitors to present to your group while they ask you questions and you have a discussion. \n",
    "\n",
    "\n",
    "**Deployment:** Follow this [link](https://docs.streamlit.io/deploy) to learn how to deploy your web app. Remember we (TA and I) will need the link to view your web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=#33AA44>Problem #5 (10pts)</font>\n",
    "![overfit](https://allmodelsarewrong.github.io/allmodelsarewrong_files/figure-html/plotoverfit6-1.png)\n",
    "\n",
    "In this problem you are going to explore fitting to different models. In doing so, you will learn about feature engineering, using the `PolynomialFeatures` library I mentioned in the lecture. You will also explore the bias-variance tradeoff. \n",
    "\n",
    "That's a lot! \n",
    "\n",
    "To simply do this, I'll give you the base code. First, run, understand and _heavily_ comment this code. This will teach you how all of these ideas come together. Then, write in a new markdown cell a detailed summary of what the code does and what trends you saw. Vary the parameters that can be and comment on what is in the plots. Add new plots or print out quantities yourself if that helps. \n",
    "\n",
    "For example, a very important take-away is this: the error in the training loss monotonically decreases as the order of the polynomial increases. Why is that? But, the test error has a much more complicated dependence. Why is that? What do both of these tell you about the importance of the idea of breaking the data into two pieces: training and testing? "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Create a polynomial of a given order\n",
    "def create_polynomial_data(order, n_samples=100):\n",
    "    np.random.seed(0)\n",
    "    X = np.linspace(0, 1, n_samples)\n",
    "    y_true = sum([np.random.randn() * (X ** i) for i in range(order + 1)])\n",
    "    return X, y_true\n",
    "\n",
    "# Step 2: Add noise to the polynomial\n",
    "def add_noise(y_true, noise_level=0.1):\n",
    "    noise = np.random.normal(0, noise_level, size=len(y_true))\n",
    "    return y_true + noise\n",
    "\n",
    "# Step 3: Fit polynomial and plot\n",
    "def fit_and_plot(X, y, order):\n",
    "    poly = PolynomialFeatures(degree=order)\n",
    "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    y_pred = model.predict(X_poly)\n",
    "    \n",
    "    # Plot original noisy data and fitted polynomial\n",
    "    plt.scatter(X, y, label='Noisy Data', color='gray', alpha=0.6)\n",
    "    plt.plot(X, y_pred, label=f'Polynomial Order {order}', color='red')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model, poly\n",
    "\n",
    "# Step 4: Fit on training set and calculate MSE on test set\n",
    "def train_test_evaluate(X, y, order, test_size=0.5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=order)\n",
    "    X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
    "    X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    # Calculate MSE on both train and test sets\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    return mse_train, mse_test\n",
    "\n",
    "# Step 5: Vary polynomial order and plot MSE vs order\n",
    "def bias_variance_tradeoff(X, y, max_order):\n",
    "    mse_train_list, mse_test_list = [], []\n",
    "    orders = range(2, max_order + 1)\n",
    "    \n",
    "    for order in orders:\n",
    "        mse_train, mse_test = train_test_evaluate(X, y, order)\n",
    "        mse_train_list.append(mse_train)\n",
    "        mse_test_list.append(mse_test)\n",
    "    \n",
    "    # Plot MSE vs polynomial order\n",
    "    plt.plot(orders, mse_train_list, label='Train MSE', marker='o')\n",
    "    plt.plot(orders, mse_test_list, label='Test MSE', marker='o')\n",
    "    plt.xlabel('Polynomial Order')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Bias-Variance Tradeoff')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Step 6: Putting it all together\n",
    "\n",
    "# Generate dataset\n",
    "order_true = 4  # True polynomial order\n",
    "X, y_true = create_polynomial_data(order_true)\n",
    "y_noisy = add_noise(y_true, noise_level=0.7)\n",
    "\n",
    "# Plot the polynomial fit for the true model and noisy data\n",
    "fit_and_plot(X, y_noisy, order_true)\n",
    "\n",
    "# Analyze the bias-variance tradeoff\n",
    "max_order = 10  # Vary the polynomial degree up to 10\n",
    "bias_variance_tradeoff(X, y_noisy, max_order)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations, you're done!\n",
    "\n",
    "### 🛑 IMPORTANT: Before Submission\n",
    "\n",
    "**STOP!** Before submitting your notebook, please follow these steps:\n",
    "\n",
    "### Step 1: Restart Your Kernel\n",
    "\n",
    "**Jupyter Notebook (Classic):**\n",
    "- Go to `Kernel` → `Restart & Clear Output`\n",
    "\n",
    "**JupyterLab:**\n",
    "- Go to `Kernel` → `Restart Kernel and Clear All Outputs...`\n",
    "\n",
    "**VS Code:**\n",
    "- Open Command Palette (`Ctrl+Shift+P` / `Cmd+Shift+P`)\n",
    "- Type \"Jupyter: Restart\" and select `Jupyter: Restart`\n",
    "- Then select `Clear All Output`\n",
    "\n",
    "**PyCharm:**\n",
    "- Go to `Run` → `Restart Kernel` (or click the restart button in the toolbar)\n",
    "- Go to `Cell` → `All Output` → `Clear`\n",
    "\n",
    "### Step 2: Run All Cells\n",
    "\n",
    "**Jupyter Notebook (Classic):**\n",
    "- Go to `Cell` → `Run All`\n",
    "\n",
    "**JupyterLab:**\n",
    "- Go to `Run` → `Run All Cells`\n",
    "\n",
    "**VS Code:**\n",
    "- Open Command Palette (`Ctrl+Shift+P` / `Cmd+Shift+P`)\n",
    "- Type \"Jupyter: Run All\" and select `Jupyter: Run All Cells`\n",
    "\n",
    "**PyCharm:**\n",
    "- Go to `Cell` → `Run All` (or use `Ctrl+Shift+F10` / `Cmd+Shift+R`)\n",
    "\n",
    "### Step 3: Verify Everything Works\n",
    "\n",
    "Check that all cells execute without errors and produce the expected output\n",
    "\n",
    "This ensures that your notebook will run correctly when graded and that you haven't missed any dependencies or variable definitions that might exist only in your current session.\n",
    "\n",
    "*Your submission will be tested in a fresh environment, so this step is crucial for full credit!*\n",
    "\n",
    "\n",
    "### Step 4: Submit \n",
    "Submit this assignment by uploading your notebook to the course Desire2Learn web page.  Go to the \"Homework\" folder, find the appropriate submission link, and upload everything there. Make sure your name is on it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2025, Department of Computational Mathematics, Science and Engineering at Michigan State University."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
